{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üé® AI Creative Platform - Backend on Google Colab\n",
                "\n",
                "This notebook runs your AI Creative Platform backend on Google Colab with GPU acceleration.\n",
                "\n",
                "**Made by Rohith Cherukuri**\n",
                "\n",
                "## üöÄ Features:\n",
                "- ‚úÖ GPU-accelerated AI processing (10x faster than CPU)\n",
                "- ‚úÖ ControlNet + Stable Diffusion\n",
                "- ‚úÖ Public URL via ngrok\n",
                "- ‚úÖ Connects to your local frontend\n",
                "\n",
                "## üìã Instructions:\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí T4\n",
                "2. **Run all cells** in order\n",
                "3. **Copy the ngrok URL** from the last cell\n",
                "4. **Update your local frontend** to use the Colab backend URL\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## üîß Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "check_gpu"
            },
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "import subprocess\n",
                "\n",
                "print(\"üéÆ GPU Check:\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"‚úÖ CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"üìä VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
                "else:\n",
                "    print(\"‚ùå No GPU detected - Make sure to enable GPU in Runtime settings!\")\n",
                "\n",
                "print(f\"\\nüêç Python: {torch.version.__version__}\")\n",
                "print(f\"üî• PyTorch: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "print(\"üì¶ Installing dependencies...\")\n",
                "\n",
                "!pip install -q fastapi==0.109.2\n",
                "!pip install -q uvicorn[standard]==0.27.1\n",
                "!pip install -q python-multipart==0.0.9\n",
                "!pip install -q python-dotenv==1.0.1\n",
                "!pip install -q Pillow==10.2.0\n",
                "!pip install -q pydantic==2.6.1\n",
                "!pip install -q aiofiles==23.2.1\n",
                "\n",
                "# AI/ML Dependencies - GPU versions\n",
                "!pip install -q diffusers==0.26.3\n",
                "!pip install -q transformers==4.38.2\n",
                "!pip install -q accelerate==0.27.2\n",
                "!pip install -q controlnet-aux==0.0.10\n",
                "!pip install -q opencv-python==4.9.0.80\n",
                "!pip install -q safetensors==0.4.2\n",
                "!pip install -q huggingface-hub==0.20.3\n",
                "!pip install -q requests==2.31.0\n",
                "\n",
                "# Install ngrok for public URL\n",
                "!pip install -q pyngrok\n",
                "\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "code_setup"
            },
            "source": [
                "## üìÅ Setup Project Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_structure"
            },
            "outputs": [],
            "source": [
                "# Create project structure\n",
                "import os\n",
                "\n",
                "# Create directories\n",
                "os.makedirs('backend/routes', exist_ok=True)\n",
                "os.makedirs('backend/utils', exist_ok=True)\n",
                "os.makedirs('backend/config', exist_ok=True)\n",
                "os.makedirs('uploads', exist_ok=True)\n",
                "\n",
                "print(\"üìÅ Project structure created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_config"
            },
            "outputs": [],
            "source": [
                "# Create AI configuration\n",
                "config_code = '''\n",
                "import os\n",
                "from typing import Dict, Any\n",
                "\n",
                "# Generation parameters\n",
                "DEFAULT_GENERATION_PARAMS = {\n",
                "    \"num_inference_steps\": 15,  # Reduced for faster generation\n",
                "    \"guidance_scale\": 7.5,\n",
                "    \"controlnet_conditioning_scale\": 1.0,\n",
                "    \"width\": 512,\n",
                "    \"height\": 512\n",
                "}\n",
                "\n",
                "# Quality enhancement prompts\n",
                "QUALITY_ENHANCERS = [\n",
                "    \"high quality\",\n",
                "    \"detailed\", \n",
                "    \"beautiful\",\n",
                "    \"masterpiece\",\n",
                "    \"best quality\"\n",
                "]\n",
                "\n",
                "NEGATIVE_PROMPTS = [\n",
                "    \"low quality\",\n",
                "    \"blurry\",\n",
                "    \"ugly\", \n",
                "    \"bad anatomy\",\n",
                "    \"worst quality\"\n",
                "]\n",
                "\n",
                "# Style-specific enhancements\n",
                "STYLE_ENHANCEMENTS = {\n",
                "    \"watercolor\": [\n",
                "        \"watercolor painting\",\n",
                "        \"soft brushstrokes\", \n",
                "        \"artistic\",\n",
                "        \"flowing colors\"\n",
                "    ],\n",
                "    \"digital_art\": [\n",
                "        \"digital art\",\n",
                "        \"vibrant colors\",\n",
                "        \"modern\",\n",
                "        \"digital painting\"\n",
                "    ],\n",
                "    \"minimalist\": [\n",
                "        \"minimalist\",\n",
                "        \"clean lines\",\n",
                "        \"simple\",\n",
                "        \"modern design\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "def get_ai_config() -> Dict[str, Any]:\n",
                "    return {\n",
                "        \"use_ai_models\": True,\n",
                "        \"device\": \"cuda\",  # Force GPU on Colab\n",
                "        \"controlnet_model\": \"lllyasviel/sd-controlnet-canny\",\n",
                "        \"stable_diffusion_model\": \"runwayml/stable-diffusion-v1-5\",\n",
                "        \"generation_params\": DEFAULT_GENERATION_PARAMS\n",
                "    }\n",
                "'''\n",
                "\n",
                "with open('backend/config/ai_config.py', 'w') as f:\n",
                "    f.write(config_code)\n",
                "\n",
                "# Create __init__.py files\n",
                "with open('backend/config/__init__.py', 'w') as f:\n",
                "    f.write('# Config package')\n",
                "\n",
                "with open('backend/utils/__init__.py', 'w') as f:\n",
                "    f.write('# Utils package')\n",
                "\n",
                "with open('backend/routes/__init__.py', 'w') as f:\n",
                "    f.write('# Routes package')\n",
                "\n",
                "print(\"‚öôÔ∏è Configuration created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_image_processor"
            },
            "outputs": [],
            "source": [
                "# Create optimized image processor for Colab\n",
                "processor_code = '''\n",
                "from PIL import Image, ImageDraw, ImageFilter, ImageEnhance, ImageOps\n",
                "import torch\n",
                "import numpy as np\n",
                "from typing import Tuple, Optional\n",
                "import cv2\n",
                "import os\n",
                "import logging\n",
                "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
                "from controlnet_aux import CannyDetector\n",
                "from config.ai_config import get_ai_config, STYLE_ENHANCEMENTS, QUALITY_ENHANCERS, NEGATIVE_PROMPTS\n",
                "\n",
                "# Set up logging\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "class ImageProcessor:\n",
                "    \"\"\"Colab-optimized image processor with GPU acceleration\"\"\"\n",
                "    \n",
                "    _instance = None\n",
                "    _models_loaded = False\n",
                "    _controlnet_canny = None\n",
                "    _pipe_canny = None\n",
                "    _canny_detector = None\n",
                "    \n",
                "    def __new__(cls):\n",
                "        if cls._instance is None:\n",
                "            cls._instance = super(ImageProcessor, cls).__new__(cls)\n",
                "        return cls._instance\n",
                "    \n",
                "    def __init__(self):\n",
                "        if hasattr(self, '_initialized'):\n",
                "            return\n",
                "        \n",
                "        self.config = get_ai_config()\n",
                "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "        \n",
                "        logger.info(f\"üéÆ Using device: {self.device}\")\n",
                "        if self.device == \"cuda\":\n",
                "            logger.info(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
                "        \n",
                "        self.pastel_colors = [\n",
                "            (230, 230, 250), (255, 182, 193), (176, 224, 230),\n",
                "            (240, 255, 240), (255, 218, 185), (221, 160, 221), (255, 255, 224)\n",
                "        ]\n",
                "        \n",
                "        self._initialized = True\n",
                "    \n",
                "    def _load_controlnet_models(self):\n",
                "        \"\"\"Load models with GPU optimization\"\"\"\n",
                "        if ImageProcessor._models_loaded:\n",
                "            return True\n",
                "        \n",
                "        try:\n",
                "            logger.info(\"üì• Loading ControlNet models...\")\n",
                "            \n",
                "            # Load ControlNet\n",
                "            ImageProcessor._controlnet_canny = ControlNetModel.from_pretrained(\n",
                "                \"lllyasviel/sd-controlnet-canny\",\n",
                "                torch_dtype=torch.float16\n",
                "            )\n",
                "            \n",
                "            # Load pipeline\n",
                "            ImageProcessor._pipe_canny = StableDiffusionControlNetPipeline.from_pretrained(\n",
                "                \"runwayml/stable-diffusion-v1-5\",\n",
                "                controlnet=ImageProcessor._controlnet_canny,\n",
                "                torch_dtype=torch.float16,\n",
                "                safety_checker=None,\n",
                "                requires_safety_checker=False\n",
                "            )\n",
                "            \n",
                "            ImageProcessor._pipe_canny.to(self.device)\n",
                "            ImageProcessor._pipe_canny.enable_model_cpu_offload()\n",
                "            \n",
                "            # Load preprocessor\n",
                "            ImageProcessor._canny_detector = CannyDetector()\n",
                "            \n",
                "            ImageProcessor._models_loaded = True\n",
                "            logger.info(\"‚úÖ Models loaded successfully!\")\n",
                "            return True\n",
                "            \n",
                "        except Exception as e:\n",
                "            logger.error(f\"‚ùå Failed to load models: {e}\")\n",
                "            return False\n",
                "    \n",
                "    def apply_style_transformation(self, image: Image.Image, prompt: str, generation_id: str) -> Image.Image:\n",
                "        \"\"\"Apply AI transformation with fallback\"\"\"\n",
                "        try:\n",
                "            if image.mode != 'RGB':\n",
                "                image = image.convert('RGB')\n",
                "            \n",
                "            # Resize for ControlNet\n",
                "            image = self._resize_for_controlnet(image, 512)\n",
                "            \n",
                "            # Try AI generation\n",
                "            result = self._generate_with_controlnet(image, prompt)\n",
                "            if result is not None:\n",
                "                logger.info(f\"‚úÖ Generated with AI for {generation_id}\")\n",
                "                return result\n",
                "            \n",
                "            # Fallback\n",
                "            logger.warning(\"‚ö†Ô∏è Using fallback processing\")\n",
                "            return self._apply_fallback_processing(image, prompt)\n",
                "            \n",
                "        except Exception as e:\n",
                "            logger.error(f\"‚ùå Error: {e}\")\n",
                "            return self._apply_fallback_processing(image, prompt)\n",
                "    \n",
                "    def _resize_for_controlnet(self, image: Image.Image, target_size: int = 512) -> Image.Image:\n",
                "        width, height = image.size\n",
                "        if width > height:\n",
                "            new_width = target_size\n",
                "            new_height = int((height * target_size) / width)\n",
                "        else:\n",
                "            new_height = target_size\n",
                "            new_width = int((width * target_size) / height)\n",
                "        \n",
                "        new_width = (new_width // 8) * 8\n",
                "        new_height = (new_height // 8) * 8\n",
                "        \n",
                "        return image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
                "    \n",
                "    def _generate_with_controlnet(self, image: Image.Image, prompt: str) -> Optional[Image.Image]:\n",
                "        \"\"\"Generate with ControlNet\"\"\"\n",
                "        try:\n",
                "            if not self._load_controlnet_models():\n",
                "                return None\n",
                "            \n",
                "            enhanced_prompt = self._enhance_prompt(prompt)\n",
                "            control_image = ImageProcessor._canny_detector(image)\n",
                "            \n",
                "            logger.info(f\"üé® Generating: {enhanced_prompt[:50]}...\")\n",
                "            \n",
                "            result = ImageProcessor._pipe_canny(\n",
                "                prompt=enhanced_prompt,\n",
                "                image=control_image,\n",
                "                num_inference_steps=15,  # Faster generation\n",
                "                guidance_scale=7.5,\n",
                "                controlnet_conditioning_scale=1.0,\n",
                "                generator=torch.Generator(device=self.device).manual_seed(42)\n",
                "            ).images[0]\n",
                "            \n",
                "            return result\n",
                "            \n",
                "        except Exception as e:\n",
                "            logger.error(f\"‚ùå ControlNet failed: {e}\")\n",
                "            return None\n",
                "    \n",
                "    def _enhance_prompt(self, prompt: str) -> str:\n",
                "        quality_terms = \", \".join(QUALITY_ENHANCERS)\n",
                "        return f\"{prompt}, {quality_terms}\"\n",
                "    \n",
                "    def _apply_fallback_processing(self, image: Image.Image, prompt: str) -> Image.Image:\n",
                "        \"\"\"Fast fallback processing\"\"\"\n",
                "        # Apply artistic filter\n",
                "        enhanced = ImageEnhance.Color(image).enhance(1.2)\n",
                "        softened = enhanced.filter(ImageFilter.GaussianBlur(radius=1))\n",
                "        return softened\n",
                "'''\n",
                "\n",
                "with open('backend/utils/image_processor.py', 'w') as f:\n",
                "    f.write(processor_code)\n",
                "\n",
                "print(\"üñºÔ∏è Image processor created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_routes"
            },
            "outputs": [],
            "source": [
                "# Create API routes\n",
                "routes_code = '''\n",
                "from fastapi import APIRouter, HTTPException\n",
                "from pydantic import BaseModel\n",
                "import base64\n",
                "import io\n",
                "import os\n",
                "import uuid\n",
                "from PIL import Image\n",
                "from datetime import datetime\n",
                "from utils.image_processor import ImageProcessor\n",
                "\n",
                "router = APIRouter()\n",
                "\n",
                "class DesignRequest(BaseModel):\n",
                "    prompt: str\n",
                "    sketch: str\n",
                "\n",
                "class DesignResponse(BaseModel):\n",
                "    image_url: str\n",
                "    generation_id: str\n",
                "    prompt_used: str\n",
                "    processing_time: float\n",
                "\n",
                "@router.post(\"/generate-design\", response_model=DesignResponse)\n",
                "async def generate_design(request: DesignRequest):\n",
                "    \"\"\"Generate design with GPU acceleration\"\"\"\n",
                "    try:\n",
                "        start_time = datetime.now()\n",
                "        \n",
                "        if not request.prompt.strip():\n",
                "            raise HTTPException(status_code=400, detail=\"Prompt required\")\n",
                "        \n",
                "        if not request.sketch:\n",
                "            raise HTTPException(status_code=400, detail=\"Sketch required\")\n",
                "        \n",
                "        # Process sketch\n",
                "        processor = ImageProcessor()\n",
                "        \n",
                "        # Decode image\n",
                "        try:\n",
                "            if request.sketch.startswith('data:image'):\n",
                "                request.sketch = request.sketch.split(',')[1]\n",
                "            \n",
                "            image_data = base64.b64decode(request.sketch)\n",
                "            sketch_image = Image.open(io.BytesIO(image_data))\n",
                "        except Exception as e:\n",
                "            raise HTTPException(status_code=400, detail=f\"Invalid image: {e}\")\n",
                "        \n",
                "        # Generate\n",
                "        generation_id = str(uuid.uuid4())\n",
                "        processed_image = processor.apply_style_transformation(\n",
                "            sketch_image, request.prompt, generation_id\n",
                "        )\n",
                "        \n",
                "        # Save\n",
                "        filename = f\"generated_{generation_id}.png\"\n",
                "        filepath = os.path.join(\"uploads\", filename)\n",
                "        processed_image.save(filepath, \"PNG\", quality=95)\n",
                "        \n",
                "        processing_time = (datetime.now() - start_time).total_seconds()\n",
                "        \n",
                "        return DesignResponse(\n",
                "            image_url=f\"/uploads/{filename}\",\n",
                "            generation_id=generation_id,\n",
                "            prompt_used=request.prompt,\n",
                "            processing_time=processing_time\n",
                "        )\n",
                "        \n",
                "    except HTTPException:\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        raise HTTPException(status_code=500, detail=f\"Generation failed: {e}\")\n",
                "'''\n",
                "\n",
                "with open('backend/routes/design.py', 'w') as f:\n",
                "    f.write(routes_code)\n",
                "\n",
                "print(\"üõ£Ô∏è Routes created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_main"
            },
            "outputs": [],
            "source": [
                "# Create main FastAPI app\n",
                "main_code = '''\n",
                "from fastapi import FastAPI\n",
                "from fastapi.middleware.cors import CORSMiddleware\n",
                "from fastapi.staticfiles import StaticFiles\n",
                "import os\n",
                "from routes.design import router as design_router\n",
                "\n",
                "app = FastAPI(\n",
                "    title=\"AI Creative Platform - Colab Backend\",\n",
                "    description=\"GPU-accelerated backend running on Google Colab\",\n",
                "    version=\"1.0.0\"\n",
                ")\n",
                "\n",
                "# Enable CORS for all origins (Colab setup)\n",
                "app.add_middleware(\n",
                "    CORSMiddleware,\n",
                "    allow_origins=[\"*\"],\n",
                "    allow_credentials=True,\n",
                "    allow_methods=[\"*\"],\n",
                "    allow_headers=[\"*\"],\n",
                ")\n",
                "\n",
                "# Mount uploads\n",
                "os.makedirs(\"uploads\", exist_ok=True)\n",
                "app.mount(\"/uploads\", StaticFiles(directory=\"uploads\"), name=\"uploads\")\n",
                "\n",
                "# Include routes\n",
                "app.include_router(design_router)\n",
                "\n",
                "@app.get(\"/\")\n",
                "async def root():\n",
                "    return {\n",
                "        \"message\": \"üé® AI Creative Platform - Colab Backend\",\n",
                "        \"status\": \"running\",\n",
                "        \"gpu_available\": \"cuda\" in str(os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")),\n",
                "        \"docs\": \"/docs\"\n",
                "    }\n",
                "\n",
                "@app.get(\"/health\")\n",
                "async def health():\n",
                "    return {\"status\": \"ok\", \"message\": \"Colab backend running! üöÄ\"}\n",
                "'''\n",
                "\n",
                "with open('backend/main.py', 'w') as f:\n",
                "    f.write(main_code)\n",
                "\n",
                "print(\"üöÄ Main app created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "run_server"
            },
            "source": [
                "## üåê Start Server with Public URL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup_ngrok"
            },
            "outputs": [],
            "source": [
                "# Setup ngrok for public URL\n",
                "from pyngrok import ngrok\n",
                "import getpass\n",
                "\n",
                "# Get ngrok auth token (optional but recommended)\n",
                "print(\"üîë Ngrok Setup (Optional but recommended):\")\n",
                "print(\"1. Go to https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
                "print(\"2. Copy your authtoken\")\n",
                "print(\"3. Paste it below (or press Enter to skip)\")\n",
                "\n",
                "auth_token = getpass.getpass(\"Ngrok authtoken (optional): \")\n",
                "\n",
                "if auth_token.strip():\n",
                "    ngrok.set_auth_token(auth_token)\n",
                "    print(\"‚úÖ Ngrok authenticated!\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Using ngrok without auth (2-hour limit)\")\n",
                "\n",
                "# Kill any existing ngrok tunnels\n",
                "ngrok.kill()\n",
                "\n",
                "print(\"üåê Ngrok ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "start_server"
            },
            "outputs": [],
            "source": [
                "# Start the FastAPI server with ngrok using threading\n",
                "import uvicorn\n",
                "import threading\n",
                "from pyngrok import ngrok\n",
                "import os\n",
                "import nest_asyncio\n",
                "\n",
                "nest_asyncio.apply()\n",
                "\n",
                "# Ensure we are in the correct directory before changing to backend\n",
                "os.chdir('/content/')\n",
                "os.chdir('backend')\n",
                "\n",
                "# Start ngrok tunnel\n",
                "try:\n",
                "    tunnels = ngrok.get_tunnels()\n",
                "    for tunnel in tunnels:\n",
                "        if tunnel.public_url.endswith(\":8000\"):\n",
                "            print(f\"Killing existing ngrok tunnel: {tunnel.public_url}\")\n",
                "            ngrok.disconnect(tunnel.public_url)\n",
                "except Exception as e:\n",
                "    print(f\"Error checking/killing tunnels: {e}\")\n",
                "\n",
                "public_url = ngrok.connect(8000)\n",
                "print(f\"\\nüåç PUBLIC URL: {public_url}\")\n",
                "print(f\"üìã Copy this URL to your local frontend!\")\n",
                "print(f\"üîó API Docs: {public_url}/docs\")\n",
                "print(f\"‚ù§Ô∏è Health Check: {public_url}/health\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üé® AI CREATIVE PLATFORM - COLAB BACKEND\")\n",
                "print(\"Made by Rohith Cherukuri\")\n",
                "print(\"=\"*60)\n",
                "print(f\"üöÄ Server starting on: {public_url}\")\n",
                "print(\"üí° Update your local frontend to use this URL\")\n",
                "print(\"üîÑ Server will restart if this cell is re-run\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "# Function to run the server in a separate thread\n",
                "def run_server():\n",
                "    uvicorn.run(\n",
                "        \"main:app\",\n",
                "        host=\"0.0.0.0\",\n",
                "        port=8000,\n",
                "        reload=False,\n",
                "        log_level=\"info\"\n",
                "    )\n",
                "\n",
                "# Start server in a daemon thread\n",
                "server_thread = threading.Thread(target=run_server, daemon=True)\n",
                "server_thread.start()\n",
                "\n",
                "print(\"‚úÖ Server is running in the background!\")\n",
                "print(\"üìù You can continue working while the server runs.\")\n",
                "print(\"‚èπÔ∏è  To stop the server, interrupt the kernel.\")\n",
                "\n",
                "# Keep the main thread alive\n",
                "try:\n",
                "    while True:\n",
                "        time.sleep(1)\n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\nüõë Server stopped by user\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "instructions"
            },
            "source": [
                "## üìã Next Steps\n",
                "\n",
                "1. **Copy the ngrok URL** from above\n",
                "2. **Update your local frontend** `.env` file:\n",
                "   ```\n",
                "   VITE_API_URL=https://your-ngrok-url.ngrok.io\n",
                "   ```\n",
                "3. **Start your local frontend**:\n",
                "   ```bash\n",
                "   npm run dev\n",
                "   ```\n",
                "4. **Enjoy GPU-accelerated AI generation!** üöÄ\n",
                "\n",
                "## üí° Tips:\n",
                "- Keep this Colab tab open while using the app\n",
                "- GPU generation is ~10x faster than CPU\n",
                "- Free Colab has usage limits - consider Colab Pro for heavy use\n",
                "- The ngrok URL changes each time you restart\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "machine_shape": "hm",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
