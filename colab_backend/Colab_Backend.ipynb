{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé® AI-Powered Doodle Recognition & Styling - Colab Backend\n",
        "\n",
        "This notebook provides a complete backend for the AI-Powered Doodle Recognition platform running on Google Colab with GPU acceleration.\n",
        "\n",
        "## Features:\n",
        "- ü§ñ **MobileNetV2/V3** for efficient doodle recognition\n",
        "- üé® **Stable Diffusion** with ControlNet for image generation\n",
        "- üöÄ **FastAPI** server with ngrok tunneling\n",
        "- üìä **Training pipeline** for custom models\n",
        "- ü§ó **Hugging Face** integration ready\n",
        "\n",
        "## Quick Start:\n",
        "1. Set your `NGROK_AUTH_TOKEN` in Colab Secrets\n",
        "2. Run all cells in order\n",
        "3. Use the public URL to connect your frontend\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository if not already available\n",
        "import os\n",
        "if not os.path.exists('colab_backend'):\n",
        "    !git clone https://github.com/RohithCherukuri816/AI-Powered-Creative-Intelligence-Platform.git\n",
        "    %cd AI-Powered-Creative-Intelligence-Platform\n",
        "else:\n",
        "    print(\"‚úÖ Repository already available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required dependencies\n",
        "!pip install -q -r colab_backend/requirements.txt\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check system capabilities\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"üñ•Ô∏è  System Information:\")\n",
        "print(f\"   Python: {sys.version}\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "print(f\"   CPU Count: {os.cpu_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ MobileNet Training (Optional)\n",
        "\n",
        "Train a custom MobileNet model on Quick Draw dataset. Skip this section if you want to use the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import training modules\n",
        "from colab_backend.mobilenet_trainer import MobileNetTrainer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = MobileNetTrainer(model_name=\"mobilenetv3_large_100\", num_classes=10)  # Using 10 classes for demo\n",
        "print(\"‚úÖ Trainer initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Quick Draw data (subset for demo)\n",
        "categories = ['car', 'cat', 'dog', 'house', 'tree', 'airplane', 'bicycle', 'bird', 'flower', 'fish']\n",
        "images, labels, class_names = trainer.download_quickdraw_data(categories=categories, samples_per_class=500)\n",
        "\n",
        "print(f\"‚úÖ Downloaded {len(images)} images across {len(class_names)} categories\")\n",
        "print(f\"   Image shape: {images[0].shape}\")\n",
        "print(f\"   Categories: {class_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data loaders\n",
        "trainer.prepare_data(images, labels, batch_size=32)\n",
        "print(\"‚úÖ Data loaders prepared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train model\n",
        "trainer.create_model(pretrained=True)\n",
        "print(\"‚úÖ Model created with transfer learning\")\n",
        "\n",
        "# Train for a few epochs (increase for better results)\n",
        "trainer.train(epochs=5, learning_rate=0.001)\n",
        "print(\"‚úÖ Training completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "accuracy, predictions, targets = trainer.evaluate()\n",
        "print(f\"‚úÖ Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "trainer.plot_training_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model for Hugging Face\n",
        "model_dir = trainer.save_model_for_huggingface(\"mobilenet_doodle_model\")\n",
        "print(f\"‚úÖ Model saved to {model_dir}\")\n",
        "\n",
        "# Optionally upload to Hugging Face Hub\n",
        "# from huggingface_hub import HfApi\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=model_dir,\n",
        "#     repo_id=\"your-username/doodle-recognition-mobilenet\",\n",
        "#     repo_type=\"model\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Start FastAPI Server\n",
        "\n",
        "Launch the complete backend server with MobileNet recognition and Stable Diffusion generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set Hugging Face token if available\n",
        "try:\n",
        "    hf_token = userdata.get('HUGGINGFACE_TOKEN')\n",
        "    os.environ['HUGGINGFACE_TOKEN'] = hf_token\n",
        "    print(\"‚úÖ Hugging Face token set\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Hugging Face token not found in secrets\")\n",
        "\n",
        "# Configure CORS for frontend connection\n",
        "os.environ['CORS_ALLOWED_ORIGINS'] = 'http://localhost:5173,http://localhost:5174'\n",
        "print(\"‚úÖ Environment configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the backend components\n",
        "from colab_backend.mobilenet_recognizer import MobileNetDoodleRecognizer\n",
        "from colab_backend.processor import ImageProcessor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Initialize components\n",
        "recognizer = MobileNetDoodleRecognizer()\n",
        "processor = ImageProcessor()\n",
        "\n",
        "print(\"‚úÖ Backend components initialized\")\n",
        "print(f\"   Model info: {recognizer.get_model_info()}\")\n",
        "print(f\"   Device: {processor.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup ngrok tunnel\n",
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Get ngrok token from Colab Secrets\n",
        "try:\n",
        "    ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    ngrok.set_auth_token(ngrok_token)\n",
        "    print(\"‚úÖ Ngrok authenticated\")\n",
        "except:\n",
        "    print(\"‚ùå NGROK_AUTH_TOKEN not found in Colab Secrets!\")\n",
        "    print(\"   Please add your ngrok token to Colab Secrets with key 'NGROK_AUTH_TOKEN'\")\n",
        "    print(\"   Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "    raise SystemExit(\"Ngrok token required\")\n",
        "\n",
        "# Create public tunnel\n",
        "public_url = ngrok.connect(8000, 'http')\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "print(f\"üìö API Docs: {public_url}/docs\")\n",
        "print(f\"üíö Health Check: {public_url}/health\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch FastAPI server\n",
        "print(\"üöÄ Starting FastAPI server...\")\n",
        "print(\"   The server will run in the background\")\n",
        "print(\"   Use the public URL above to connect your frontend\")\n",
        "print(\"   Press Ctrl+C in the terminal to stop the server\")\n",
        "\n",
        "# Start server process\n",
        "proc = subprocess.Popen([\n",
        "    sys.executable, \n",
        "    '-m', \n",
        "    'uvicorn', \n",
        "    'colab_backend.app:app', \n",
        "    '--host', '0.0.0.0', \n",
        "    '--port', '8000',\n",
        "    '--reload'\n",
        "])\n",
        "\n",
        "print(f\"‚úÖ Server started with PID: {proc.pid}\")\n",
        "print(f\"üîó Connect your frontend to: {public_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Test the API\n",
        "\n",
        "Test the running server to ensure everything works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test API endpoints\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "base_url = str(public_url)\n",
        "\n",
        "# Test health endpoint\n",
        "try:\n",
        "    response = requests.get(f\"{base_url}/health\")\n",
        "    print(f\"‚úÖ Health check: {response.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Health check failed: {e}\")\n",
        "\n",
        "# Test model info endpoint\n",
        "try:\n",
        "    response = requests.get(f\"{base_url}/model-info\")\n",
        "    print(f\"‚úÖ Model info: {response.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model info failed: {e}\")\n",
        "\n",
        "# Test supported labels\n",
        "try:\n",
        "    response = requests.get(f\"{base_url}/supported-labels\")\n",
        "    data = response.json()\n",
        "    print(f\"‚úÖ Supported labels: {data['total_categories']} categories\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Supported labels failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Monitor Server\n",
        "\n",
        "Monitor the server status and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor server status\n",
        "import psutil\n",
        "import GPUtil\n",
        "\n",
        "def show_system_status():\n",
        "    print(\"üìä System Status:\")\n",
        "    \n",
        "    # CPU usage\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    print(f\"   CPU Usage: {cpu_percent}%\")\n",
        "    \n",
        "    # Memory usage\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"   RAM Usage: {memory.percent}% ({memory.used/1e9:.1f}GB / {memory.total/1e9:.1f}GB)\")\n",
        "    \n",
        "    # GPU usage\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
        "        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"   GPU Memory: {gpu_memory:.1f}GB / {gpu_memory_total:.1f}GB\")\n",
        "    \n",
        "    # Server process\n",
        "    if proc.poll() is None:\n",
        "        print(f\"   Server Status: ‚úÖ Running (PID: {proc.pid})\")\n",
        "    else:\n",
        "        print(f\"   Server Status: ‚ùå Stopped\")\n",
        "    \n",
        "    print(f\"   Public URL: {public_url}\")\n",
        "\n",
        "show_system_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Server management commands\n",
        "print(\"üõ†Ô∏è  Server Management:\")\n",
        "print(f\"   Public URL: {public_url}\")\n",
        "print(f\"   API Documentation: {public_url}/docs\")\n",
        "print(f\"   Server PID: {proc.pid}\")\n",
        "print(\"\")\n",
        "print(\"üìù Usage Instructions:\")\n",
        "print(\"   1. Copy the public URL above\")\n",
        "print(\"   2. Update your frontend .env file: VITE_API_URL=<public_url>\")\n",
        "print(\"   3. Start your frontend: npm run dev\")\n",
        "print(\"   4. Draw doodles and generate art!\")\n",
        "print(\"\")\n",
        "print(\"‚ö†Ô∏è  Note: The server will stop when this Colab session ends\")\n",
        "print(\"   Keep this notebook running while using the application\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõë Stop Server\n",
        "\n",
        "Run this cell to stop the server when you're done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the server\n",
        "try:\n",
        "    proc.terminate()\n",
        "    proc.wait(timeout=10)\n",
        "    print(\"‚úÖ Server stopped gracefully\")\n",
        "except:\n",
        "    proc.kill()\n",
        "    print(\"‚ö†Ô∏è  Server force stopped\")\n",
        "\n",
        "# Close ngrok tunnel\n",
        "ngrok.disconnect(public_url)\n",
        "print(\"‚úÖ Ngrok tunnel closed\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}